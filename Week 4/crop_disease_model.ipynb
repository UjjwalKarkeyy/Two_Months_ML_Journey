{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363fd7f7",
   "metadata": {},
   "source": [
    "## **Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efa06b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 17:18:38.806729: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 17:18:38.918769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 17:18:42.027447: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e4a26",
   "metadata": {},
   "source": [
    "## Verifying the GPU Setup [tensorflow + CUDA (Compute Unified Device Architecture) => Let's you use GPU for Computation other than Graphics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781937c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-20 17:18:44.197046: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 17:18:44.285834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 17:18:46.479449: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa59ff",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "#### Used *'https://youtube.com/playlist?list=PLvz5lCwTgdXDNcXEVwwHsb9DwjNXZGsoy&si=gPCwVkKpS63FVLAC'* for guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ed928",
   "metadata": {},
   "source": [
    "# Training Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b686a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/root123/GitHub/Two_Months_ML_Journey/Week 4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abab9437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24652 files belonging to 15 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760960031.408355  190601 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "training_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    './data/PlantVillage_Cleaned/split_data/train',\n",
    "    labels=\"inferred\", # leaving inferred, keras generates labels and class name automatically i.e.,\n",
    "    # [Pepper_mold, Potato_healthy, Tomato_healthy, ...] alphabetically, where labels [0, 1, 2, ...]\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=40,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    pad_to_aspect_ratio=False,\n",
    "    data_format=None,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f299cb",
   "metadata": {},
   "source": [
    "## Validation Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41367da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5278 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    './data/PlantVillage_Cleaned/split_data/valid',\n",
    "    labels=\"inferred\", \n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=40,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    pad_to_aspect_ratio=False,\n",
    "    data_format=None,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b565fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[111. 102. 107.]\n",
      "   [ 97.  88.  93.]\n",
      "   [111. 102. 107.]\n",
      "   ...\n",
      "   [111. 102. 107.]\n",
      "   [105.  96. 101.]\n",
      "   [ 82.  73.  78.]]\n",
      "\n",
      "  [[123. 114. 119.]\n",
      "   [108.  99. 104.]\n",
      "   [115. 106. 111.]\n",
      "   ...\n",
      "   [124. 115. 120.]\n",
      "   [105.  96. 101.]\n",
      "   [ 86.  77.  82.]]\n",
      "\n",
      "  [[123. 114. 119.]\n",
      "   [111. 102. 107.]\n",
      "   [115. 106. 111.]\n",
      "   ...\n",
      "   [114. 105. 110.]\n",
      "   [100.  91.  96.]\n",
      "   [111. 102. 107.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[168. 163. 169.]\n",
      "   [165. 160. 166.]\n",
      "   [161. 156. 162.]\n",
      "   ...\n",
      "   [161. 154. 161.]\n",
      "   [159. 152. 159.]\n",
      "   [156. 149. 156.]]\n",
      "\n",
      "  [[163. 158. 164.]\n",
      "   [160. 155. 161.]\n",
      "   [157. 152. 158.]\n",
      "   ...\n",
      "   [161. 154. 161.]\n",
      "   [161. 154. 161.]\n",
      "   [160. 153. 160.]]\n",
      "\n",
      "  [[152. 147. 153.]\n",
      "   [151. 146. 152.]\n",
      "   [152. 147. 153.]\n",
      "   ...\n",
      "   [160. 153. 160.]\n",
      "   [162. 155. 162.]\n",
      "   [164. 157. 164.]]]\n",
      "\n",
      "\n",
      " [[[  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   ...\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]]\n",
      "\n",
      "  [[  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   ...\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]]\n",
      "\n",
      "  [[  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   ...\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   ...\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]]\n",
      "\n",
      "  [[  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   ...\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]]\n",
      "\n",
      "  [[  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   ...\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]\n",
      "   [  3.   3.   3.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 15.   0.   0.]\n",
      "   [ 96. 103.  95.]\n",
      "   [105. 161. 148.]\n",
      "   ...\n",
      "   [187. 195. 206.]\n",
      "   [ 13.   7.  21.]\n",
      "   [121. 107. 122.]]\n",
      "\n",
      "  [[113.  87.  86.]\n",
      "   [ 36.  33.  28.]\n",
      "   [ 10.  50.  41.]\n",
      "   ...\n",
      "   [143. 164. 157.]\n",
      "   [ 17.  17.  19.]\n",
      "   [ 15.   4.  10.]]\n",
      "\n",
      "  [[ 84.  44.  52.]\n",
      "   [ 82.  57.  61.]\n",
      "   [ 42.  48.  46.]\n",
      "   ...\n",
      "   [107. 153. 107.]\n",
      "   [ 61.  75.  52.]\n",
      "   [183. 181. 169.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 49.  25.  77.]\n",
      "   [ 39.  25.  50.]\n",
      "   [ 24.  31.   0.]\n",
      "   ...\n",
      "   [ 22.   6.   0.]\n",
      "   [ 49.  80.  49.]\n",
      "   [ 15.  71.  44.]]\n",
      "\n",
      "  [[ 96.  67.  87.]\n",
      "   [  3.   0.   9.]\n",
      "   [ 64. 107. 100.]\n",
      "   ...\n",
      "   [  0.  18.   0.]\n",
      "   [ 36. 106.  46.]\n",
      "   [  0.  74.   6.]]\n",
      "\n",
      "  [[ 25.   0.   0.]\n",
      "   [ 28.  27.  32.]\n",
      "   [ 66. 127. 132.]\n",
      "   ...\n",
      "   [ 99. 160. 118.]\n",
      "   [ 71. 161.  87.]\n",
      "   [ 41. 145.  56.]]]\n",
      "\n",
      "\n",
      " [[[132. 130. 135.]\n",
      "   [130. 128. 133.]\n",
      "   [128. 126. 131.]\n",
      "   ...\n",
      "   [129. 127. 130.]\n",
      "   [128. 126. 129.]\n",
      "   [127. 125. 128.]]\n",
      "\n",
      "  [[130. 128. 133.]\n",
      "   [128. 126. 131.]\n",
      "   [127. 125. 130.]\n",
      "   ...\n",
      "   [130. 128. 131.]\n",
      "   [129. 127. 130.]\n",
      "   [129. 127. 130.]]\n",
      "\n",
      "  [[127. 125. 130.]\n",
      "   [126. 124. 129.]\n",
      "   [126. 124. 129.]\n",
      "   ...\n",
      "   [129. 127. 130.]\n",
      "   [130. 128. 131.]\n",
      "   [129. 127. 130.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 83.  87.  90.]\n",
      "   [ 83.  87.  90.]\n",
      "   [ 83.  87.  90.]\n",
      "   ...\n",
      "   [ 78.  83.  86.]\n",
      "   [ 78.  83.  86.]\n",
      "   [ 80.  85.  88.]]\n",
      "\n",
      "  [[ 82.  86.  89.]\n",
      "   [ 83.  87.  90.]\n",
      "   [ 83.  87.  90.]\n",
      "   ...\n",
      "   [ 78.  83.  86.]\n",
      "   [ 78.  83.  86.]\n",
      "   [ 80.  85.  88.]]\n",
      "\n",
      "  [[ 82.  86.  89.]\n",
      "   [ 82.  86.  89.]\n",
      "   [ 83.  87.  90.]\n",
      "   ...\n",
      "   [ 78.  83.  86.]\n",
      "   [ 78.  83.  86.]\n",
      "   [ 80.  85.  88.]]]\n",
      "\n",
      "\n",
      " [[[172. 170. 171.]\n",
      "   [159. 157. 158.]\n",
      "   [153. 151. 152.]\n",
      "   ...\n",
      "   [136. 134. 137.]\n",
      "   [137. 135. 138.]\n",
      "   [142. 140. 143.]]\n",
      "\n",
      "  [[167. 165. 166.]\n",
      "   [161. 159. 160.]\n",
      "   [161. 159. 160.]\n",
      "   ...\n",
      "   [149. 147. 150.]\n",
      "   [153. 151. 154.]\n",
      "   [160. 158. 161.]]\n",
      "\n",
      "  [[167. 165. 166.]\n",
      "   [165. 163. 164.]\n",
      "   [168. 166. 167.]\n",
      "   ...\n",
      "   [151. 149. 152.]\n",
      "   [159. 157. 160.]\n",
      "   [168. 166. 169.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[111. 107. 104.]\n",
      "   [113. 109. 106.]\n",
      "   [115. 111. 108.]\n",
      "   ...\n",
      "   [124. 120. 117.]\n",
      "   [113. 109. 106.]\n",
      "   [125. 121. 118.]]\n",
      "\n",
      "  [[118. 114. 111.]\n",
      "   [112. 108. 105.]\n",
      "   [113. 109. 106.]\n",
      "   ...\n",
      "   [125. 121. 118.]\n",
      "   [118. 114. 111.]\n",
      "   [134. 130. 127.]]\n",
      "\n",
      "  [[115. 111. 108.]\n",
      "   [103.  99.  96.]\n",
      "   [105. 101.  98.]\n",
      "   ...\n",
      "   [119. 115. 112.]\n",
      "   [112. 108. 105.]\n",
      "   [130. 126. 123.]]]], shape=(32, 256, 256, 3), dtype=float32) (32, 256, 256, 3)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(32, 15), dtype=float32) (32, 15)\n"
     ]
    }
   ],
   "source": [
    "for x, y in training_set:\n",
    "    print(x, x.shape)\n",
    "    print(y, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5dda8",
   "metadata": {},
   "source": [
    "# **Building Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8686b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe39a7a",
   "metadata": {},
   "source": [
    "## Building Convolution Layers (Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0d8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  CNN Concept Summary:\n",
    "\n",
    "# 1ï¸âƒ£ Each cnn.add() = one layer (Conv, Pool, etc.)\n",
    "#     â†’ Conv2D = learns features (edges â†’ textures â†’ shapes)\n",
    "#     â†’ MaxPool2D = downsamples feature maps (keeps strongest features)\n",
    "\n",
    "# 2ï¸âƒ£ Repeated Conv+Pool blocks = deeper understanding:\n",
    "#     Block1: basic edges/colors\n",
    "#     Block2: shapes/textures\n",
    "#     Block3: complex leaf patterns\n",
    "#     Block4: disease regions\n",
    "\n",
    "# 3ï¸âƒ£ Kernel (3x3): small filter sliding over input\n",
    "#     â†’ detects local patterns (like edge fragments)\n",
    "\n",
    "# 4ï¸âƒ£ padding='same': adds zero borders â†’ output height & width stay same\n",
    "#     padding='valid': no padding â†’ output shrinks\n",
    "\n",
    "# 5ï¸âƒ£ input_shape: only needed in the first Conv2D layer\n",
    "#     â†’ later layers infer input size automatically\n",
    "\n",
    "# 6ï¸âƒ£ Max Pooling (2x2): reduces feature map size by half\n",
    "#     â†’ keeps key activations, adds translation invariance\n",
    "\n",
    "# 7ï¸âƒ£ Hierarchy summary:\n",
    "#     Input â†’ Conv â†’ Pool â†’ Conv â†’ Pool â†’ Flatten â†’ Dense â†’ Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77009e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = [256,256, 3])) # input_size = [widht, height, input shape]\n",
    "cnn.add(Conv2D(filters=32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "cnn.add(MaxPool2D(pool_size = 2, strides = 2)) # pool size -> shape of pooling filter, strides -> movement of sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11bfcf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=64, kernel_size = 3, padding = 'same', activation = 'relu')) \n",
    "cnn.add(Conv2D(filters=64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "cnn.add(MaxPool2D(pool_size = 2, strides = 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5cebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=128, kernel_size = 3, padding = 'same', activation = 'relu')) \n",
    "cnn.add(Conv2D(filters=128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "cnn.add(MaxPool2D(pool_size = 2, strides = 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde8f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=256, kernel_size = 3, padding = 'same', activation = 'relu')) \n",
    "cnn.add(Conv2D(filters=256, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "cnn.add(MaxPool2D(pool_size = 2, strides = 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c89c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the results to pass to the dense layer\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43db7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units = 1024, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ef7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "cnn.add(Dense(units = 15, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629a387",
   "metadata": {},
   "source": [
    "## Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fdc649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6344a79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65536</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">67,109,888</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,950</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚         \u001b[38;5;34m9,248\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   â”‚        \u001b[38;5;34m36,928\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚       \u001b[38;5;34m147,584\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚       \u001b[38;5;34m295,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚       \u001b[38;5;34m590,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65536\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           â”‚    \u001b[38;5;34m67,109,888\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)             â”‚        \u001b[38;5;34m38,950\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             â”‚           \u001b[38;5;34m585\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,321,679</span> (260.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m68,321,679\u001b[0m (260.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,321,679</span> (260.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m68,321,679\u001b[0m (260.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195d6af",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "558beabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 17:30:16.766164: I external/local_xla/xla/service/service.cc:163] XLA service 0x77faf0005f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-20 17:30:16.766215: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-10-20 17:30:16.849403: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-20 17:30:17.410905: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n",
      "2025-10-20 17:30:17.542643: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-20 17:30:18.009394: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_751', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-20 17:30:19.794991: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_751', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-10-20 17:30:19.980722: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1174', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-20 17:30:20.376630: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1174', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "2025-10-20 17:30:21.445322: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 528.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:21.450817: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 528.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:21.450868: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 528.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:21.512696: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 528.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:21.512767: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 528.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:21.512800: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 528.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:22.434992: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:23.030800: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:23.304031: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-10-20 17:30:24.016586: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "I0000 00:00:1760960741.843742  190804 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-10-20 17:30:51.847623: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.54GiB (rounded to 1654690816)requested by op \n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-10-20 17:30:51.847672: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1049] BFCAllocator dump for GPU_0_bfc\n",
      "2025-10-20 17:30:51.847682: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (256): \tTotal Chunks: 50, Chunks in use: 50. 12.5KiB allocated for chunks. 12.5KiB in use in bin. 1.7KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847684: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (512): \tTotal Chunks: 4, Chunks in use: 4. 2.2KiB allocated for chunks. 2.2KiB in use in bin. 2.1KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847687: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1024): \tTotal Chunks: 4, Chunks in use: 4. 4.5KiB allocated for chunks. 4.5KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847689: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2048): \tTotal Chunks: 4, Chunks in use: 4. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 9.7KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847692: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4096): \tTotal Chunks: 3, Chunks in use: 3. 13.0KiB allocated for chunks. 13.0KiB in use in bin. 11.4KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847695: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847698: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847700: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (32768): \tTotal Chunks: 2, Chunks in use: 2. 85.2KiB allocated for chunks. 85.2KiB in use in bin. 72.0KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847702: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (65536): \tTotal Chunks: 4, Chunks in use: 2. 284.5KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847705: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (131072): \tTotal Chunks: 4, Chunks in use: 4. 592.0KiB allocated for chunks. 592.0KiB in use in bin. 592.0KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847707: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (262144): \tTotal Chunks: 2, Chunks in use: 2. 576.0KiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847709: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (524288): \tTotal Chunks: 2, Chunks in use: 2. 1.54MiB allocated for chunks. 1.54MiB in use in bin. 1.12MiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847711: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1048576): \tTotal Chunks: 2, Chunks in use: 2. 2.25MiB allocated for chunks. 2.25MiB in use in bin. 2.25MiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847712: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2097152): \tTotal Chunks: 3, Chunks in use: 2. 6.60MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847714: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847716: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847721: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 24.00MiB allocated for chunks. 24.00MiB in use in bin. 24.00MiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847723: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847725: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847726: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 232.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847728: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 2. 1.46GiB allocated for chunks. 512.00MiB in use in bin. 512.00MiB client-requested in use in bin.\n",
      "2025-10-20 17:30:51.847730: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1072] Bin for 1.54GiB was 256.00MiB, Chunk State: \n",
      "2025-10-20 17:30:51.847736: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1078]   Size: 987.47MiB | Requested Size: 18.25MiB | in_use: 0 | bin_num: 20, prev:   Size: 256.00MiB | Requested Size: 256.00MiB | in_use: 1 | bin_num: -1\n",
      "2025-10-20 17:30:51.847738: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 1853253120\n",
      "2025-10-20 17:30:51.847742: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00000 of size 1280 next 1\n",
      "2025-10-20 17:30:51.847745: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00500 of size 256 next 2\n",
      "2025-10-20 17:30:51.847748: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00600 of size 256 next 3\n",
      "2025-10-20 17:30:51.847751: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00700 of size 256 next 4\n",
      "2025-10-20 17:30:51.847753: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00800 of size 256 next 5\n",
      "2025-10-20 17:30:51.847755: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00900 of size 256 next 6\n",
      "2025-10-20 17:30:51.847757: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00a00 of size 256 next 7\n",
      "2025-10-20 17:30:51.847758: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00b00 of size 256 next 8\n",
      "2025-10-20 17:30:51.847760: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00c00 of size 256 next 9\n",
      "2025-10-20 17:30:51.847761: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00d00 of size 256 next 10\n",
      "2025-10-20 17:30:51.847763: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00e00 of size 256 next 11\n",
      "2025-10-20 17:30:51.847764: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e00f00 of size 256 next 12\n",
      "2025-10-20 17:30:51.847765: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01000 of size 256 next 13\n",
      "2025-10-20 17:30:51.847767: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01100 of size 256 next 14\n",
      "2025-10-20 17:30:51.847768: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01200 of size 256 next 15\n",
      "2025-10-20 17:30:51.847769: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01300 of size 256 next 17\n",
      "2025-10-20 17:30:51.847771: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01400 of size 256 next 18\n",
      "2025-10-20 17:30:51.847772: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01500 of size 256 next 16\n",
      "2025-10-20 17:30:51.847774: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01600 of size 256 next 19\n",
      "2025-10-20 17:30:51.847775: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01700 of size 256 next 24\n",
      "2025-10-20 17:30:51.847777: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01800 of size 256 next 22\n",
      "2025-10-20 17:30:51.847778: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01900 of size 256 next 23\n",
      "2025-10-20 17:30:51.847779: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01a00 of size 256 next 27\n",
      "2025-10-20 17:30:51.847781: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01b00 of size 256 next 28\n",
      "2025-10-20 17:30:51.847782: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01c00 of size 256 next 31\n",
      "2025-10-20 17:30:51.847783: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01d00 of size 256 next 34\n",
      "2025-10-20 17:30:51.847785: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01e00 of size 256 next 32\n",
      "2025-10-20 17:30:51.847786: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e01f00 of size 512 next 33\n",
      "2025-10-20 17:30:51.847788: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02100 of size 256 next 37\n",
      "2025-10-20 17:30:51.847789: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02200 of size 256 next 38\n",
      "2025-10-20 17:30:51.847790: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02300 of size 512 next 41\n",
      "2025-10-20 17:30:51.847792: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02500 of size 256 next 44\n",
      "2025-10-20 17:30:51.847793: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02600 of size 256 next 42\n",
      "2025-10-20 17:30:51.847795: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02700 of size 1024 next 43\n",
      "2025-10-20 17:30:51.847796: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02b00 of size 256 next 47\n",
      "2025-10-20 17:30:51.847797: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02c00 of size 256 next 48\n",
      "2025-10-20 17:30:51.847799: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e02d00 of size 1280 next 20\n",
      "2025-10-20 17:30:51.847800: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e03200 of size 3584 next 21\n",
      "2025-10-20 17:30:51.847802: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e04000 of size 256 next 51\n",
      "2025-10-20 17:30:51.847803: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e04100 of size 256 next 54\n",
      "2025-10-20 17:30:51.847804: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e04200 of size 4096 next 52\n",
      "2025-10-20 17:30:51.847806: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05200 of size 256 next 58\n",
      "2025-10-20 17:30:51.847807: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05300 of size 256 next 53\n",
      "2025-10-20 17:30:51.847809: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05400 of size 256 next 57\n",
      "2025-10-20 17:30:51.847810: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05500 of size 256 next 61\n",
      "2025-10-20 17:30:51.847811: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05600 of size 256 next 62\n",
      "2025-10-20 17:30:51.847813: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05700 of size 256 next 64\n",
      "2025-10-20 17:30:51.847814: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05800 of size 256 next 65\n",
      "2025-10-20 17:30:51.847816: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05900 of size 256 next 66\n",
      "2025-10-20 17:30:51.847818: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05a00 of size 256 next 67\n",
      "2025-10-20 17:30:51.847819: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05b00 of size 256 next 69\n",
      "2025-10-20 17:30:51.847820: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e05c00 of size 5120 next 72\n",
      "2025-10-20 17:30:51.847822: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e07000 of size 2304 next 73\n",
      "2025-10-20 17:30:51.847824: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e07900 of size 512 next 68\n",
      "2025-10-20 17:30:51.847825: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e07b00 of size 1024 next 74\n",
      "2025-10-20 17:30:51.847827: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e07f00 of size 4096 next 76\n",
      "2025-10-20 17:30:51.847828: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e08f00 of size 256 next 78\n",
      "2025-10-20 17:30:51.847830: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e09000 of size 2304 next 79\n",
      "2025-10-20 17:30:51.847831: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e09900 of size 256 next 80\n",
      "2025-10-20 17:30:51.847832: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e09a00 of size 256 next 81\n",
      "2025-10-20 17:30:51.847834: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e09b00 of size 50432 next 26\n",
      "2025-10-20 17:30:51.847836: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e16000 of size 36864 next 25\n",
      "2025-10-20 17:30:51.847837: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e1f000 of size 256 next 82\n",
      "2025-10-20 17:30:51.847839: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e1f100 of size 2048 next 84\n",
      "2025-10-20 17:30:51.847841: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e1f900 of size 768 next 90\n",
      "2025-10-20 17:30:51.847842: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e1fc00 of size 256 next 86\n",
      "2025-10-20 17:30:51.847843: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e1fd00 of size 256 next 91\n",
      "2025-10-20 17:30:51.847845: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 503e1fe00 of size 70144 next 30\n",
      "2025-10-20 17:30:51.847846: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e31000 of size 73728 next 29\n",
      "2025-10-20 17:30:51.847848: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e43000 of size 147456 next 70\n",
      "2025-10-20 17:30:51.847849: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e67000 of size 73728 next 83\n",
      "2025-10-20 17:30:51.847851: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 503e79000 of size 73728 next 36\n",
      "2025-10-20 17:30:51.847852: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503e8b000 of size 147456 next 35\n",
      "2025-10-20 17:30:51.847853: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503eaf000 of size 294912 next 40\n",
      "2025-10-20 17:30:51.847855: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503ef7000 of size 294912 next 39\n",
      "2025-10-20 17:30:51.847857: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503f3f000 of size 155648 next 63\n",
      "2025-10-20 17:30:51.847858: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 503f65000 of size 1024000 next 46\n",
      "2025-10-20 17:30:51.847860: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 50405f000 of size 589824 next 45\n",
      "2025-10-20 17:30:51.847862: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 5040ef000 of size 1179648 next 50\n",
      "2025-10-20 17:30:51.847863: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 50420f000 of size 1179648 next 49\n",
      "2025-10-20 17:30:51.847865: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 50432f000 of size 2359296 next 71\n",
      "2025-10-20 17:30:51.847866: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 50456f000 of size 155648 next 77\n",
      "2025-10-20 17:30:51.847868: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 504595000 of size 2203648 next 56\n",
      "2025-10-20 17:30:51.847869: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 5047af000 of size 2359296 next 55\n",
      "2025-10-20 17:30:51.847870: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 5049ef000 of size 268435456 next 75\n",
      "2025-10-20 17:30:51.847872: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 5149ef000 of size 25165824 next 85\n",
      "2025-10-20 17:30:51.847873: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 5161ef000 of size 243269632 next 60\n",
      "2025-10-20 17:30:51.847874: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 5249ef000 of size 268435456 next 59\n",
      "2025-10-20 17:30:51.847876: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 5349ef000 of size 1035433472 next 18446744073709551615\n",
      "2025-10-20 17:30:51.847878: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1110]      Summary of in-use Chunks by size: \n",
      "2025-10-20 17:30:51.847880: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 50 Chunks of size 256 totalling 12.5KiB\n",
      "2025-10-20 17:30:51.847882: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 3 Chunks of size 512 totalling 1.5KiB\n",
      "2025-10-20 17:30:51.847883: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 768 totalling 768B\n",
      "2025-10-20 17:30:51.847885: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 1024 totalling 2.0KiB\n",
      "2025-10-20 17:30:51.847886: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 1280 totalling 2.5KiB\n",
      "2025-10-20 17:30:51.847887: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 2048 totalling 2.0KiB\n",
      "2025-10-20 17:30:51.847889: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 2304 totalling 4.5KiB\n",
      "2025-10-20 17:30:51.847890: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 3584 totalling 3.5KiB\n",
      "2025-10-20 17:30:51.847892: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 4096 totalling 8.0KiB\n",
      "2025-10-20 17:30:51.847893: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 5120 totalling 5.0KiB\n",
      "2025-10-20 17:30:51.847895: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 36864 totalling 36.0KiB\n",
      "2025-10-20 17:30:51.847896: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 50432 totalling 49.2KiB\n",
      "2025-10-20 17:30:51.847898: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 73728 totalling 144.0KiB\n",
      "2025-10-20 17:30:51.847899: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 147456 totalling 288.0KiB\n",
      "2025-10-20 17:30:51.847901: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 155648 totalling 304.0KiB\n",
      "2025-10-20 17:30:51.847902: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 294912 totalling 576.0KiB\n",
      "2025-10-20 17:30:51.847904: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 589824 totalling 576.0KiB\n",
      "2025-10-20 17:30:51.847905: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 1024000 totalling 1000.0KiB\n",
      "2025-10-20 17:30:51.847907: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 1179648 totalling 2.25MiB\n",
      "2025-10-20 17:30:51.847908: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 2359296 totalling 4.50MiB\n",
      "2025-10-20 17:30:51.847910: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 25165824 totalling 24.00MiB\n",
      "2025-10-20 17:30:51.847912: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 268435456 totalling 512.00MiB\n",
      "2025-10-20 17:30:51.847914: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] Sum Total of in-use chunks: 545.69MiB\n",
      "2025-10-20 17:30:51.847916: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119] Total bytes in pool: 1853253120 memory_limit_: 1853253223 available bytes: 103 curr_region_allocation_bytes_: 3706506752\n",
      "2025-10-20 17:30:51.847919: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1124] Stats: \n",
      "Limit:                      1853253223\n",
      "InUse:                       572202496\n",
      "MaxInUse:                   1683139328\n",
      "NumAllocs:                        1048\n",
      "MaxAllocSize:                750220800\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-10-20 17:30:51.847922: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] *****************____________****************_______________________________________________________\n",
      "2025-10-20 17:30:51.848075: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1654690568 bytes.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 701, in shell_main\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 469, in dispatch_shell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 379, in execute_request\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 899, in execute_request\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 471, in do_execute\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 632, in run_cell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_190601/1706687424.py\", line 1, in <module>\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1654690568 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_4210]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 701, in shell_main\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 469, in dispatch_shell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 379, in execute_request\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 899, in execute_request\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 471, in do_execute\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 632, in run_cell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_190601/1706687424.py\", line 1, in <module>\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/root123/GitHub/Two_Months_ML_Journey/Week 4/tf/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1654690568 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_4210]"
     ]
    }
   ],
   "source": [
    "training_history = cnn.fit(x = training_set, validation_data = validation_set, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355372e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
